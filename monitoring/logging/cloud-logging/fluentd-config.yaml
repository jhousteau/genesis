# Fluentd Configuration for Google Cloud Logging Integration
# Collects, processes, and forwards logs from all platform applications

apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
  namespace: monitoring
data:
  fluent.conf: |
    # Main configuration file
    @include sources.conf
    @include filters.conf
    @include outputs.conf
    
  sources.conf: |
    # Kubernetes container logs
    <source>
      @type tail
      @id in_tail_container_logs
      path /var/log/containers/*.log
      pos_file /var/log/fluentd-containers.log.pos
      tag raw.kubernetes.*
      read_from_head true
      <parse>
        @type multi_format
        <pattern>
          format json
          time_key time
          time_format %Y-%m-%dT%H:%M:%S.%NZ
        </pattern>
        <pattern>
          format /^(?<time>.+) (?<stream>stdout|stderr) [^ ]* (?<log>.*)$/
          time_format %Y-%m-%dT%H:%M:%S.%N%:z
        </pattern>
      </parse>
    </source>

    # Kubernetes system logs
    <source>
      @type tail
      @id in_tail_kubelet
      multiline_flush_interval 5s
      path /var/log/kubelet.log
      pos_file /var/log/fluentd-kubelet.log.pos
      tag kubelet
      <parse>
        @type kubernetes
        expression /^(?<severity>\w)(?<time>\d{4} \d{1,2}:\d{2}:\d{2}\.\d+)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/m
        time_format %m%d %H:%M:%S.%N
      </parse>
    </source>

    # Application logs from mounted volumes
    <source>
      @type tail
      @id in_tail_app_logs
      path /var/log/apps/**/*.log
      pos_file /var/log/fluentd-apps.log.pos
      tag app.*
      <parse>
        @type json
        time_key timestamp
        time_format %Y-%m-%dT%H:%M:%S.%NZ
      </parse>
    </source>

    # Nginx access logs
    <source>
      @type tail
      @id in_tail_nginx_access
      path /var/log/nginx/access.log
      pos_file /var/log/fluentd-nginx-access.log.pos
      tag nginx.access
      <parse>
        @type nginx
      </parse>
    </source>

    # System logs
    <source>
      @type systemd
      @id in_systemd_docker
      matches [{"_SYSTEMD_UNIT": "docker.service"}]
      read_from_head true
      tag systemd.docker
    </source>

    <source>
      @type systemd
      @id in_systemd_kubelet
      matches [{"_SYSTEMD_UNIT": "kubelet.service"}]
      read_from_head true
      tag systemd.kubelet
    </source>

  filters.conf: |
    # Parse Kubernetes metadata
    <filter raw.kubernetes.**>
      @type kubernetes_metadata
      @id filter_kube_metadata
      kubernetes_url "#{ENV['KUBERNETES_SERVICE_HOST']}:#{ENV['KUBERNETES_SERVICE_PORT_HTTPS']}"
      verify_ssl "#{ENV['KUBERNETES_VERIFY_SSL'] || true}"
      ca_file "#{ENV['KUBERNETES_CA_FILE']}"
      skip_labels false
      skip_container_metadata false
      skip_master_url false
      skip_namespace_metadata false
    </filter>

    # Parse JSON logs and extract structured fields
    <filter raw.kubernetes.**>
      @type parser
      @id filter_parser
      key_name log
      reserve_data true
      remove_key_name_field true
      <parse>
        @type multi_format
        <pattern>
          format json
          time_key timestamp
          keep_time_key true
        </pattern>
        <pattern>
          format none
        </pattern>
      </parse>
    </filter>

    # Add environment and service information
    <filter raw.kubernetes.**>
      @type record_transformer
      @id filter_add_service_info
      <record>
        environment "#{ENV['ENVIRONMENT'] || 'unknown'}"
        platform "universal-project-platform"
        cluster_name "#{ENV['CLUSTER_NAME'] || 'unknown'}"
        node_name "#{ENV['NODE_NAME']}"
        pod_name "#{ENV['POD_NAME']}"
      </record>
    </filter>

    # Extract service name from Kubernetes labels
    <filter raw.kubernetes.**>
      @type record_transformer
      @id filter_service_name
      <record>
        service_name ${record.dig("kubernetes", "labels", "app") || record.dig("kubernetes", "labels", "k8s-app") || "unknown"}
        service_version ${record.dig("kubernetes", "labels", "version") || "unknown"}
      </record>
    </filter>

    # Security log filtering and enhancement
    <filter **>
      @type grep
      @id filter_security_events
      <regexp>
        key message
        pattern (authentication|authorization|login|logout|access denied|security|breach|attack|intrusion)
      </regexp>
    </filter>

    <filter **>
      @type record_transformer
      @id filter_security_enhancement
      enable_ruby true
      <record>
        security_event true
        threat_level ${
          case record["message"]
          when /attack|breach|intrusion/i then "high"
          when /access denied|unauthorized/i then "medium"
          else "low"
          end
        }
      </record>
    </filter>

    # Performance log processing
    <filter **>
      @type record_transformer
      @id filter_performance_metrics
      enable_ruby true
      <record>
        has_performance_data ${!record["duration"].nil? || !record["memory_usage"].nil?}
      </record>
    </filter>

    # Error log enhancement
    <filter **>
      @type record_transformer
      @id filter_error_enhancement
      enable_ruby true
      <record>
        error_level ${
          case record["level"]
          when /ERROR|CRITICAL|FATAL/i then record["level"]
          when /exception|error|fail/i then "ERROR"
          else nil
          end
        }
      </record>
    </filter>

    # Rate limiting to prevent log flooding
    <filter **>
      @type throttle
      @id filter_throttle
      group_key kubernetes.pod_name
      group_bucket_period_s 60
      group_bucket_limit 1000
    </filter>

    # Remove sensitive information
    <filter **>
      @type record_modifier
      @id filter_remove_sensitive
      remove_keys password,token,secret,key,auth
    </filter>

    # Deduplicate logs
    <filter **>
      @type dedupe
      @id filter_dedupe
      key message,kubernetes.pod_name
      cache_ttl 300
    </filter>

  outputs.conf: |
    # Google Cloud Logging output
    <match **>
      @type google_cloud
      @id out_google_cloud
      project_id "#{ENV['GCP_PROJECT']}"
      zone "#{ENV['GCP_ZONE']}"
      vm_id "#{ENV['VM_ID']}"
      vm_name "#{ENV['VM_NAME']}"
      
      # Use structured logging
      use_metadata_service true
      detect_json true
      enable_monitoring true
      monitoring_type opencensus
      
      # Buffer configuration
      <buffer>
        @type file
        path /var/log/fluentd-buffers/stackdriver.buffer
        flush_mode interval
        retry_type exponential_backoff
        flush_thread_count 2
        flush_interval 5s
        retry_forever true
        retry_max_interval 30
        chunk_limit_size 2M
        queue_limit_length 8
        overflow_action block
      </buffer>
      
      # Resource and labels
      <format>
        @type json
      </format>
      
      # Kubernetes resource detection
      k8s_cluster_name "#{ENV['CLUSTER_NAME']}"
      k8s_cluster_location "#{ENV['CLUSTER_LOCATION']}"
    </match>

    # Prometheus metrics output for monitoring Fluentd itself
    <match fluent.**>
      @type prometheus
      @id out_prometheus_fluent
      <metric>
        name fluentd_input_status_num_records_total
        type counter
        desc The total number of incoming records
        <labels>
          tag ${tag}
          hostname ${hostname}
        </labels>
      </metric>
    </match>

    # Elasticsearch output (optional, for ELK stack)
    # <match **>
    #   @type elasticsearch
    #   @id out_elasticsearch
    #   host "#{ENV['ELASTICSEARCH_HOST'] || 'elasticsearch'}"
    #   port "#{ENV['ELASTICSEARCH_PORT'] || '9200'}"
    #   logstash_format true
    #   logstash_prefix universal-platform
    #   logstash_dateformat %Y.%m.%d
    #   include_tag_key true
    #   type_name _doc
    #   tag_key @log_name
    #   <buffer>
    #     flush_mode interval
    #     retry_type exponential_backoff
    #     flush_thread_count 2
    #     flush_interval 5s
    #     retry_forever true
    #     retry_max_interval 30
    #     chunk_limit_size 2M
    #     queue_limit_length 8
    #     overflow_action block
    #   </buffer>
    # </match>

---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: monitoring
  labels:
    k8s-app: fluentd-logging
    version: v1
spec:
  selector:
    matchLabels:
      k8s-app: fluentd-logging
  template:
    metadata:
      labels:
        k8s-app: fluentd-logging
        version: v1
    spec:
      serviceAccountName: fluentd
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      containers:
      - name: fluentd
        image: gcr.io/google-containers/fluentd-gcp:3.2.0
        env:
        - name: FLUENTD_SYSTEMD_CONF
          value: disable
        - name: GCP_PROJECT
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: ENVIRONMENT
          value: "production"
        - name: CLUSTER_NAME
          value: "universal-platform-cluster"
        resources:
          limits:
            memory: 512Mi
            cpu: 500m
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: config-volume
          mountPath: /etc/fluent/config.d
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: config-volume
        configMap:
          name: fluentd-config